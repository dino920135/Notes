- Sigmoid -> most common for MLPs
- RBF( Radian bases)
- [Yes you should understand backprop | by Andrej Karpathy | Medium](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b)
- ## tricks
	- input normalization
	- weight initializations
		- random but relative SMALL (related to num. of input), and prop to 1/sqrt(num of input)
	- learning rate
	- training set size
		- huge amount of training-set
		- **in navigation -> not enough -> transfer learning** (predictable behavior)
		  might not suitable for INS
	- avoid overfitting by **early stop**
	- avoid overfitting by use of **L1 or L2 regularization penalty**
	- for ConvNet (Or large network)
		- **dropout**
		  for forward prop. randomly close some of the neurals (test if the neural is important)
		- **Batch normalization** (image processing)
		  Normalization of the result from batch:
		  Loss of imformation? remove outlier?
		  ![Normalization in Deep Learning â€“ calculated | content](https://calculatedcontent.com/wp-content/uploads/2017/06/batchnorm2-e1497643748774.png){:height 269, :width 409}
- **Overfitting is some how creating stuff** (Nerf)
-