- Sigmoid -> most common for MLPs
- RBF( Radian bases)
- [Yes you should understand backprop | by Andrej Karpathy | Medium](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b)
- ## tricks
	- input normalization
	- weight initializations
		- random but relative SMALL (related to num. of input), and prop to 1/sqrt(num of input)
	- learning rate
	- training set size
		- huge amount of training-set
		- **in navigation -> not enough -> transfer learning** (predictable behavior)
		  might not suitable for INS
	- avoid overfitting by **early stop**
	- avoid overfitting by use of **L1 or L2 regularization**
	- for ConvNet
		- dropout
- **Overfitting is some how creating stuff**
-